{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183ffe5e-c3f3-43f8-84fb-c6d25e14c83a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tutorial de classificaÃ§Ã£o de imagens TensorFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326d46fd-50b0-482b-be38-ed9ee27da5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b7a46-5191-45ae-8625-6794bf5b06c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09f55d-eefd-4506-9157-462a0fc43ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd16a1-9cdf-4a96-9885-37b57ec08a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data_dir.glob(\"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac681ab-f246-4e98-8f56-5b33a809962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75737b9f-2b54-46f0-b4d9-1d3a355a9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "roses = list(data_dir.glob('roses/*'))\n",
    "PIL.Image.open(str(roses[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7507bc9-d40a-4bde-a200-8456124c3cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(str(roses[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0525e-13ec-499f-9dc4-eb9f1cba350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tulips = list(data_dir.glob('tulips/*'))\n",
    "PIL.Image.open(str(tulips[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6c3b5-1745-4bc9-bbbf-822fc597f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(str(tulips[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c6f81-b5a5-4178-9069-b0b1f60098ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7820f4-842e-4dd7-a016-4c50817d19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb9e5e-6bcd-4ab1-8bfb-bcce6573f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c3ce1-e04c-4183-96e5-ad7f0446cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02ac87-a348-48c5-9155-fac2efd6bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650682e-8e6a-43ad-8eaa-b77554f1a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36629e-f4b8-4e25-9346-03148de628ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e82e8-94a2-4a19-817b-ec6e4e1f0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2088f783-0736-4e05-b9b6-aa824e4ed498",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f19d96-d2ae-4513-85b6-6642e691df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4311e4-f1f1-4603-bc36-e2502be9780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8ac01-78ed-41a9-8689-a81b0f10ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab88f5-1fea-43ee-b14d-c697b51be8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046def2d-5c4b-46c3-848a-95e164f4dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8da61-f54c-4aac-9b5c-638c9496c909",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Criar funÃ§Ãµes para abrir, exibir, manipular e salvar imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4081c7-e423-495a-bc0f-35dd4770be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979185d-82f6-4031-8a3e-7344bcdf0d6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Flip Horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f6564-46aa-4877-9e42-fa86a551e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_horizontal(image_path, output_path):\n",
    "    \"\"\" Aplica o flip horizontal em uma imagem e salva no caminho especificado. \"\"\"\n",
    "    image_path = os.path.expanduser(image_path)  # Suporte para '~'\n",
    "    output_path = os.path.expanduser(output_path)\n",
    "    \n",
    "    # Carregar a imagem\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"âŒ ERRO: NÃ£o foi possÃ­vel carregar a imagem {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Criar pasta de saÃ­da, se nÃ£o existir\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Aplicar flip horizontal\n",
    "    flipped = cv2.flip(image, 1)\n",
    "    \n",
    "    # Salvar a imagem processada\n",
    "    success = cv2.imwrite(output_path, flipped)\n",
    "    if not success:\n",
    "        print(f\"âŒ ERRO: NÃ£o foi possÃ­vel salvar a imagem em {output_path}\")\n",
    "    else:\n",
    "        print(f\"âœ… Flip horizontal salvo em: {output_path}\")\n",
    "\n",
    "def apply_flip_to_dataset(input_dir, output_dir):\n",
    "    \"\"\" Percorre todas as imagens do dataset e aplica o flip horizontal. \"\"\"\n",
    "    input_dir = os.path.expanduser(input_dir)  # Caminho absoluto\n",
    "    output_dir = os.path.expanduser(output_dir)  # Caminho absoluto\n",
    "    \n",
    "    # Verificar se o diretÃ³rio de entrada existe\n",
    "    if not os.path.exists(input_dir):\n",
    "        print(f\"âŒ ERRO: O diretÃ³rio {input_dir} nÃ£o existe!\")\n",
    "        return\n",
    "\n",
    "    # Percorrer todas as classes do dataset\n",
    "    for class_folder in os.listdir(input_dir):\n",
    "        class_input_path = os.path.join(input_dir, class_folder)\n",
    "\n",
    "        # Ignorar arquivos, sÃ³ processar diretÃ³rios (classes)\n",
    "        if not os.path.isdir(class_input_path):\n",
    "            continue\n",
    "\n",
    "        # Criar diretÃ³rio correspondente na saÃ­da\n",
    "        class_output_path = os.path.join(output_dir, class_folder)\n",
    "        os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "        # Percorrer todas as imagens da classe\n",
    "        for image_file in os.listdir(class_input_path):\n",
    "            image_input_path = os.path.join(class_input_path, image_file)\n",
    "            image_output_path = os.path.join(class_output_path, f\"{os.path.splitext(image_file)[0]}_flip.jpg\")\n",
    "\n",
    "            # Aplicar flip horizontal\n",
    "            flip_horizontal(image_input_path, image_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9996d-2e54-4088-a291-1855316f67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar flip horizontal\n",
    "apply_flip_to_dataset(\"~/.keras/datasets/flower_photos\", \"augmented_flower_photos_flip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85699f69-8b58-4879-ab25-9177ded108aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# RotaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a9a0f9-b6a6-4ffe-a637-6a008f47b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(image_path, output_dir, max_angle):\n",
    "    \"\"\" \n",
    "    Rotaciona a imagem por um Ã¢ngulo aleatÃ³rio entre 0 e max_angle e salva no diretÃ³rio de saÃ­da.\n",
    "    Preenche pixels indefinidos com preto (RGB 0,0,0).\n",
    "    \"\"\"\n",
    "    image_path = os.path.expanduser(image_path)  # Suporte para '~'\n",
    "    output_dir = os.path.expanduser(output_dir)  # Caminho absoluto para saÃ­da\n",
    "\n",
    "    # Carregar imagem\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"âŒ ERRO: Falha ao carregar {image_path}\")\n",
    "        return\n",
    "    \n",
    "    h, w = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    angle = random.uniform(0, max_angle)  # Escolher Ã¢ngulo aleatÃ³rio entre 0 e max_angle\n",
    "\n",
    "    # Criar matriz de rotaÃ§Ã£o e aplicar a transformaÃ§Ã£o\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, rotation_matrix, (w, h), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "\n",
    "    # Criar caminho de saÃ­da mantendo a estrutura do dataset original\n",
    "    class_name = os.path.basename(os.path.dirname(image_path))  # ObtÃ©m a classe (ex: 'daisy')\n",
    "    image_name = os.path.basename(image_path)  # ObtÃ©m o nome do arquivo original\n",
    "    output_class_dir = os.path.join(output_dir, class_name)  # Criar diretÃ³rio por classe\n",
    "    os.makedirs(output_class_dir, exist_ok=True)  # Garantir que a pasta existe\n",
    "\n",
    "    # Criar nome do arquivo de saÃ­da\n",
    "    output_path = os.path.join(output_class_dir, f\"{os.path.splitext(image_name)[0]}_rot{int(angle)}.jpg\")\n",
    "\n",
    "    # Salvar a imagem rotacionada\n",
    "    success = cv2.imwrite(output_path, rotated)\n",
    "\n",
    "    print(f\"{'âœ… RotaÃ§Ã£o aplicada e salva em ' + output_path if success else 'âŒ Erro ao salvar a imagem!'}\")\n",
    "\n",
    "def apply_rotation_to_dataset(input_dir, base_output_dir, max_angles=[1, 5, 10, 15, 25, 45, 90]):\n",
    "    \"\"\" \n",
    "    Gera 7 datasets diferentes, aplicando rotaÃ§Ã£o aleatÃ³ria entre 0 e X graus. \n",
    "    \"\"\"\n",
    "    input_dir = os.path.expanduser(input_dir)  # Caminho absoluto\n",
    "    base_output_dir = os.path.expanduser(base_output_dir)  # Caminho absoluto\n",
    "\n",
    "    # Verificar se o diretÃ³rio de entrada existe\n",
    "    if not os.path.exists(input_dir):\n",
    "        print(f\"âŒ ERRO: O diretÃ³rio {input_dir} nÃ£o existe!\")\n",
    "        return\n",
    "\n",
    "    # Criar um dataset separado para cada intervalo de rotaÃ§Ã£o\n",
    "    for max_angle in max_angles:\n",
    "        output_dir = f\"{base_output_dir}_rot0_{max_angle}\"\n",
    "        print(f\"\\nðŸ“Œ Gerando dataset com rotaÃ§Ã£o entre 0 e {max_angle} graus em: {output_dir}\")\n",
    "        \n",
    "        # Percorrer todas as classes do dataset\n",
    "        for class_folder in os.listdir(input_dir):\n",
    "            class_input_path = os.path.join(input_dir, class_folder)\n",
    "\n",
    "            # Ignorar arquivos, sÃ³ processar diretÃ³rios (classes)\n",
    "            if not os.path.isdir(class_input_path):\n",
    "                continue\n",
    "\n",
    "            # Criar diretÃ³rio correspondente na saÃ­da\n",
    "            class_output_path = os.path.join(output_dir, class_folder)\n",
    "            os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "            # Percorrer todas as imagens da classe\n",
    "            for image_file in os.listdir(class_input_path):\n",
    "                image_input_path = os.path.join(class_input_path, image_file)\n",
    "                rotate_image(image_input_path, output_dir, max_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5310153-8b93-478b-a010-a345b0f10f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar rotaÃ§Ã£o e gerar os 7 datasets\n",
    "apply_rotation_to_dataset(\"~/.keras/datasets/flower_photos\", \"augmented_flower_photos_rotated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f064984-8261-4202-8895-d735f9d32512",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295fc8d1-9a96-4445-afd2-4db7a3461b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_image(image_path, output_dir, max_zoom_factor):\n",
    "    \"\"\" \n",
    "    Aplica zoom aleatÃ³rio entre 1.0 e max_zoom_factor e corta para manter as dimensÃµes originais.\n",
    "    Salva no diretÃ³rio de saÃ­da mantendo a estrutura do dataset original.\n",
    "    \"\"\"\n",
    "    image_path = os.path.expanduser(image_path)  # Suporte para '~'\n",
    "    output_dir = os.path.expanduser(output_dir)  # Caminho absoluto para saÃ­da\n",
    "\n",
    "    # Carregar imagem\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"âŒ ERRO: Falha ao carregar {image_path}\")\n",
    "        return\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "    zoom_factor = random.uniform(1.0, max_zoom_factor)  # Escolher um fator de zoom aleatÃ³rio entre 1 e max_zoom\n",
    "\n",
    "    # Calcular novas dimensÃµes de corte\n",
    "    new_h, new_w = int(h / zoom_factor), int(w / zoom_factor)\n",
    "    start_h = (h - new_h) // 2\n",
    "    start_w = (w - new_w) // 2\n",
    "\n",
    "    # Recortar e redimensionar para manter as dimensÃµes originais\n",
    "    cropped = image[start_h:start_h + new_h, start_w:start_w + new_w]\n",
    "    zoomed = cv2.resize(cropped, (w, h))\n",
    "\n",
    "    # Criar caminho de saÃ­da mantendo a estrutura do dataset original\n",
    "    class_name = os.path.basename(os.path.dirname(image_path))  # ObtÃ©m a classe (ex: 'daisy')\n",
    "    image_name = os.path.basename(image_path)  # ObtÃ©m o nome do arquivo original\n",
    "    output_class_dir = os.path.join(output_dir, class_name)  # Criar diretÃ³rio por classe\n",
    "    os.makedirs(output_class_dir, exist_ok=True)  # Garantir que a pasta existe\n",
    "\n",
    "    # Criar nome do arquivo de saÃ­da\n",
    "    output_path = os.path.join(output_class_dir, f\"{os.path.splitext(image_name)[0]}_zoom{int((zoom_factor - 1) * 100)}.jpg\")\n",
    "\n",
    "    # Salvar a imagem com zoom\n",
    "    success = cv2.imwrite(output_path, zoomed)\n",
    "\n",
    "    print(f\"{'âœ… Zoom aplicado e salvo em ' + output_path if success else 'âŒ Erro ao salvar a imagem!'}\")\n",
    "\n",
    "def apply_zoom_to_dataset(input_dir, base_output_dir, max_zoom_factors=[1.05, 1.10, 1.20, 1.40, 1.80]):\n",
    "    \"\"\" \n",
    "    Gera 5 datasets diferentes, aplicando zoom aleatÃ³rio entre 1.0 e Y% de zoom.\n",
    "    \"\"\"\n",
    "    input_dir = os.path.expanduser(input_dir)  # Caminho absoluto\n",
    "    base_output_dir = os.path.expanduser(base_output_dir)  # Caminho absoluto\n",
    "\n",
    "    # Verificar se o diretÃ³rio de entrada existe\n",
    "    if not os.path.exists(input_dir):\n",
    "        print(f\"âŒ ERRO: O diretÃ³rio {input_dir} nÃ£o existe!\")\n",
    "        return\n",
    "\n",
    "    # Criar um dataset separado para cada intervalo de zoom\n",
    "    for max_zoom in max_zoom_factors:\n",
    "        output_dir = f\"{base_output_dir}_zoom0_{int((max_zoom - 1) * 100)}\"\n",
    "        print(f\"\\nðŸ“Œ Gerando dataset com zoom entre 0% e {int((max_zoom - 1) * 100)}% em: {output_dir}\")\n",
    "\n",
    "        # Percorrer todas as classes do dataset\n",
    "        for class_folder in os.listdir(input_dir):\n",
    "            class_input_path = os.path.join(input_dir, class_folder)\n",
    "\n",
    "            # Ignorar arquivos, sÃ³ processar diretÃ³rios (classes)\n",
    "            if not os.path.isdir(class_input_path):\n",
    "                continue\n",
    "\n",
    "            # Criar diretÃ³rio correspondente na saÃ­da\n",
    "            class_output_path = os.path.join(output_dir, class_folder)\n",
    "            os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "            # Percorrer todas as imagens da classe\n",
    "            for image_file in os.listdir(class_input_path):\n",
    "                image_input_path = os.path.join(class_input_path, image_file)\n",
    "                zoom_image(image_input_path, output_dir, max_zoom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d7641-3988-4ee0-b70f-535d880da849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar zoom e gerar os 5 datasets\n",
    "apply_zoom_to_dataset(\"~/.keras/datasets/flower_photos\", \"augmented_flower_photos_zoom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7e3fd2-7698-467c-a775-ac8fb8afbfbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Incluir o Dataset original nos Datasets aumentados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68054bec-2659-4d51-abe9-78f48ead9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_original_to_augmented(original_dir, augmented_dirs):\n",
    "    \"\"\"\n",
    "    Copia todas as imagens do dataset original para os datasets aumentados,\n",
    "    garantindo que as classes originais estejam presentes nos aumentos.\n",
    "    \n",
    "    original_dir: Caminho do dataset original (ex: ~/.keras/datasets/flower_photos)\n",
    "    augmented_dirs: Lista de diretÃ³rios dos datasets aumentados\n",
    "    \"\"\"\n",
    "    original_dir = os.path.expanduser(original_dir)  # Suporte para '~'\n",
    "    augmented_dirs = [os.path.expanduser(d) for d in augmented_dirs]  # Expande caminhos\n",
    "\n",
    "    # Verifica se o diretÃ³rio original existe\n",
    "    if not os.path.exists(original_dir):\n",
    "        print(f\"ERRO: O diretÃ³rio original '{original_dir}' nÃ£o existe!\")\n",
    "        return\n",
    "    \n",
    "    # Iterar sobre cada dataset aumentado\n",
    "    for augmented_dir in augmented_dirs:\n",
    "        print(f\"\\nCopiando imagens do dataset original para: {augmented_dir}\")\n",
    "\n",
    "        # Criar diretÃ³rios das classes no dataset aumentado\n",
    "        for class_folder in os.listdir(original_dir):\n",
    "            class_path_original = os.path.join(original_dir, class_folder)\n",
    "            class_path_augmented = os.path.join(augmented_dir, class_folder)\n",
    "\n",
    "            # Verifica se Ã© um diretÃ³rio (ignora arquivos)\n",
    "            if not os.path.isdir(class_path_original):\n",
    "                continue\n",
    "\n",
    "            # Cria a pasta no dataset aumentado se nÃ£o existir\n",
    "            os.makedirs(class_path_augmented, exist_ok=True)\n",
    "\n",
    "            # Copia todas as imagens da classe original para a classe do dataset aumentado\n",
    "            for image_file in os.listdir(class_path_original):\n",
    "                src = os.path.join(class_path_original, image_file)\n",
    "                dst = os.path.join(class_path_augmented, image_file)\n",
    "\n",
    "                # Copiar somente se ainda nÃ£o estiver no dataset aumentado\n",
    "                if not os.path.exists(dst):\n",
    "                    shutil.copy2(src, dst)\n",
    "\n",
    "        print(f\"CÃ³pia concluÃ­da para {augmented_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099d412-a38c-41b7-80ad-4f77411c0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista dos diretÃ³rios aumentados\n",
    "augmented_datasets = [\n",
    "    \"augmented_flower_photos_flip\",\n",
    "    \"augmented_flower_photos_rotated_rot0_1\",\n",
    "    \"augmented_flower_photos_rotated_rot0_5\",\n",
    "    \"augmented_flower_photos_rotated_rot0_10\",\n",
    "    \"augmented_flower_photos_rotated_rot0_15\",\n",
    "    \"augmented_flower_photos_rotated_rot0_25\",\n",
    "    \"augmented_flower_photos_rotated_rot0_45\",\n",
    "    \"augmented_flower_photos_rotated_rot0_90\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_5\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_10\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_20\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_40\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_80\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f16abe-6838-469f-ae27-d1eb5e489105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar o dataset original para os datasets aumentados\n",
    "copy_original_to_augmented(\"~/.keras/datasets/flower_photos\", augmented_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a72938-626e-43d6-a96b-e10e93b5c775",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Contagem dos dados dos Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229bfb0-86b6-42b4-9e3a-13c645aeab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images_in_dataset(dataset_path, dataset_name):\n",
    "    \"\"\" Conta a quantidade total de imagens no dataset e exibe os resultados. \"\"\"\n",
    "    dataset_path = os.path.expanduser(dataset_path)  # Garante caminho absoluto\n",
    "    total_images = 0\n",
    "    class_counts = {}  # Contador por classe\n",
    "\n",
    "    # Percorre todas as pastas dentro do dataset\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "\n",
    "        # Ignorar arquivos, sÃ³ processar diretÃ³rios (classes)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        # Conta as imagens dentro da classe\n",
    "        num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        total_images += num_images\n",
    "        class_counts[class_folder] = num_images\n",
    "\n",
    "    # Exibir resultados\n",
    "    print(f\"\\n**Resumo do dataset: {dataset_name}**\")\n",
    "    print(f\"   Total de imagens: {total_images}\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"   Classe '{class_name}': {count} imagens\")\n",
    "\n",
    "    return total_images, class_counts\n",
    "\n",
    "# Contar imagens no dataset original\n",
    "original_total, original_counts = count_images_in_dataset(\"~/.keras/datasets/flower_photos\", \"Original\")\n",
    "\n",
    "# Contar imagens no dataset aumentado\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_flip\", \"Flip\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_1\", \"RotaÃ§Ã£o x a 1Âº\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_5\", \"RotaÃ§Ã£o x a 5Âº\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_10\", \"RotaÃ§Ã£o x a 10Âº\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_15\", \"RotaÃ§Ã£o x a 15Âº\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_25\", \"RotaÃ§Ã£o x a 25Âº\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_45\", \"RotaÃ§Ã£o x a 45Âº\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_90\", \"RotaÃ§Ã£o x a 90Âº\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_zoom_zoom0_5\", \"Zoom y a 5%\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_zoom_zoom0_10\", \"Zoom y a 10%\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_zoom_zoom0_19\", \"Zoom y a 20%\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_zoom_zoom0_39\", \"Zoom y a 40%\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_zoom_zoom0_80\", \"Zoom y a 80%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ab56d-55e4-4bff-abd4-a2475a9c3461",
   "metadata": {},
   "source": [
    "# TREINAMENTO DE TODOS OS DATASETS (FLIP, ROTAÃ‡ÃƒO E ZOOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "273c3e01-4c58-49bd-8570-806a852f3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c106622-fe34-4a83-8e46-d619eb5e6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de datasets para Flip, RotaÃ§Ã£o e Zoom\n",
    "datasets = [\n",
    "    \"augmented_flower_photos_flip\",\n",
    "    \"augmented_flower_photos_rotated_rot0_1\",\n",
    "    \"augmented_flower_photos_rotated_rot0_5\",\n",
    "    \"augmented_flower_photos_rotated_rot0_10\",\n",
    "    \"augmented_flower_photos_rotated_rot0_15\",\n",
    "    \"augmented_flower_photos_rotated_rot0_25\",\n",
    "    \"augmented_flower_photos_rotated_rot0_45\",\n",
    "    \"augmented_flower_photos_rotated_rot0_90\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_5\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_10\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_20\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_40\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_80\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2cfb6c-9cb1-4f7f-abec-ada33b9ce6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um dicionÃ¡rio para armazenar os histÃ³ricos de treinamento\n",
    "training_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e93424-18c8-4660-851e-a7f95a05d87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_flip\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:22:13.849172: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-02-04 22:22:13.849205: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-02-04 22:22:13.849214: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-02-04 22:22:13.849781: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-04 22:22:13.850039: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 90, 90, 16)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 45, 45, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 22, 22, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 30976)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:22:14.632766: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2299 - accuracy: 0.5043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:22:25.165607: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 12s 60ms/step - loss: 1.2299 - accuracy: 0.5043 - val_loss: 1.0129 - val_accuracy: 0.5743\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.9610 - accuracy: 0.6323 - val_loss: 0.9694 - val_accuracy: 0.6213\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.8755 - accuracy: 0.6912 - val_loss: 1.2607 - val_accuracy: 0.5756\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.8323 - accuracy: 0.7236 - val_loss: 1.6633 - val_accuracy: 0.5995\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 0.9708 - accuracy: 0.7147 - val_loss: 1.8604 - val_accuracy: 0.6015\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.9929 - accuracy: 0.7706 - val_loss: 3.2001 - val_accuracy: 0.5790\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 2.2528 - accuracy: 0.7360 - val_loss: 6.4109 - val_accuracy: 0.4966\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 3.4169 - accuracy: 0.7435 - val_loss: 10.4076 - val_accuracy: 0.6069\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 13.3519 - accuracy: 0.6851 - val_loss: 39.3122 - val_accuracy: 0.5388\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 32.4304 - accuracy: 0.7059 - val_loss: 74.4513 - val_accuracy: 0.5940\n",
      " Modelo treinado e salvo para augmented_flower_photos_flip!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moa/Desktop/tensorflow-test/env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_1 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 90, 90, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 45, 45, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 22, 22, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:24:02.491052: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.1804 - accuracy: 0.5078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:24:12.416741: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 11s 57ms/step - loss: 1.1804 - accuracy: 0.5078 - val_loss: 0.9683 - val_accuracy: 0.6022\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.9634 - accuracy: 0.6500 - val_loss: 1.1944 - val_accuracy: 0.5525\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.8304 - accuracy: 0.7079 - val_loss: 1.4036 - val_accuracy: 0.6253\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.9319 - accuracy: 0.7284 - val_loss: 1.4981 - val_accuracy: 0.6471\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.4365 - accuracy: 0.7554 - val_loss: 2.2894 - val_accuracy: 0.7064\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 10s 56ms/step - loss: 2.0599 - accuracy: 0.7670 - val_loss: 5.6601 - val_accuracy: 0.6526\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 10s 56ms/step - loss: 5.6279 - accuracy: 0.7580 - val_loss: 24.2675 - val_accuracy: 0.6396\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 18.8783 - accuracy: 0.7050 - val_loss: 37.6731 - val_accuracy: 0.6676\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 10s 56ms/step - loss: 86.1298 - accuracy: 0.6819 - val_loss: 268.2289 - val_accuracy: 0.5743\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 254.3374 - accuracy: 0.6991 - val_loss: 644.2495 - val_accuracy: 0.6219\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_1!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_5\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_2 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 90, 90, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 45, 45, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 22, 22, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:25:49.041904: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2067 - accuracy: 0.4986"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:25:59.128766: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 11s 58ms/step - loss: 1.2067 - accuracy: 0.4986 - val_loss: 0.9501 - val_accuracy: 0.6240\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.8787 - accuracy: 0.6730 - val_loss: 0.9848 - val_accuracy: 0.6144\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.7367 - accuracy: 0.7444 - val_loss: 1.0183 - val_accuracy: 0.6996\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.7737 - accuracy: 0.7856 - val_loss: 1.9454 - val_accuracy: 0.6540\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.3178 - accuracy: 0.7815 - val_loss: 3.5480 - val_accuracy: 0.6635\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 2.7308 - accuracy: 0.7638 - val_loss: 4.9710 - val_accuracy: 0.7078\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 5.1150 - accuracy: 0.7670 - val_loss: 14.9564 - val_accuracy: 0.6471\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 9.1280 - accuracy: 0.7905 - val_loss: 21.6273 - val_accuracy: 0.6975\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 20.7521 - accuracy: 0.7800 - val_loss: 55.4164 - val_accuracy: 0.6737\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 37.8027 - accuracy: 0.7967 - val_loss: 97.2537 - val_accuracy: 0.6703\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_5!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_10\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_3 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 90, 90, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:27:36.068876: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.3827 - accuracy: 0.4264"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:27:46.122590: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 11s 58ms/step - loss: 1.3827 - accuracy: 0.4264 - val_loss: 1.0668 - val_accuracy: 0.5586\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.0649 - accuracy: 0.5947 - val_loss: 1.1794 - val_accuracy: 0.5940\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.4537 - accuracy: 0.6095 - val_loss: 4.2463 - val_accuracy: 0.3583\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.0612 - accuracy: 0.6608 - val_loss: 1.1567 - val_accuracy: 0.6131\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 1.0407 - accuracy: 0.7042 - val_loss: 1.9287 - val_accuracy: 0.6042\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 3.5180 - accuracy: 0.5991 - val_loss: 2.1375 - val_accuracy: 0.6308\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 10s 56ms/step - loss: 7.9711 - accuracy: 0.5482 - val_loss: 30.6365 - val_accuracy: 0.4230\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 19.2702 - accuracy: 0.5450 - val_loss: 20.4133 - val_accuracy: 0.5245\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 59ms/step - loss: 29.5520 - accuracy: 0.5545 - val_loss: 65.9434 - val_accuracy: 0.4952\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 220.4574 - accuracy: 0.4838 - val_loss: 773.5009 - val_accuracy: 0.4646\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_10!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_15\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_4 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:29:24.341620: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.1939 - accuracy: 0.5112"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:29:34.500391: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 11s 59ms/step - loss: 1.1939 - accuracy: 0.5112 - val_loss: 1.0733 - val_accuracy: 0.5661\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 1.1635 - accuracy: 0.5567 - val_loss: 1.3312 - val_accuracy: 0.5184\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 1.3266 - accuracy: 0.6005 - val_loss: 4.7160 - val_accuracy: 0.3549\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.6628 - accuracy: 0.5662 - val_loss: 1.5293 - val_accuracy: 0.5845\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.9674 - accuracy: 0.5358 - val_loss: 2.5166 - val_accuracy: 0.4407\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 2.1178 - accuracy: 0.6008 - val_loss: 5.4276 - val_accuracy: 0.4932\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 4.5952 - accuracy: 0.5816 - val_loss: 7.1761 - val_accuracy: 0.5456\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 7.6208 - accuracy: 0.6199 - val_loss: 11.3995 - val_accuracy: 0.5831\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 7.7410 - accuracy: 0.6776 - val_loss: 21.9437 - val_accuracy: 0.5307\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 17.3644 - accuracy: 0.6403 - val_loss: 23.5795 - val_accuracy: 0.6308\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_15!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_25\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_5 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:31:13.075566: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.3261 - accuracy: 0.4557"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:31:23.139102: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 11s 58ms/step - loss: 1.3261 - accuracy: 0.4557 - val_loss: 1.1556 - val_accuracy: 0.5572\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.0087 - accuracy: 0.6075 - val_loss: 1.0204 - val_accuracy: 0.6042\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 60ms/step - loss: 0.8542 - accuracy: 0.6761 - val_loss: 0.9632 - val_accuracy: 0.6410\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 0.7096 - accuracy: 0.7347 - val_loss: 1.4571 - val_accuracy: 0.5913\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 59ms/step - loss: 0.5814 - accuracy: 0.7946 - val_loss: 1.0434 - val_accuracy: 0.6710\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 59ms/step - loss: 0.4641 - accuracy: 0.8415 - val_loss: 1.7079 - val_accuracy: 0.6199\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 11s 59ms/step - loss: 0.6590 - accuracy: 0.8161 - val_loss: 1.6455 - val_accuracy: 0.6471\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 59ms/step - loss: 0.4688 - accuracy: 0.8672 - val_loss: 2.5120 - val_accuracy: 0.6151\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.3783 - accuracy: 0.8942 - val_loss: 2.0346 - val_accuracy: 0.6757\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 0.9572 - accuracy: 0.8435 - val_loss: 3.5586 - val_accuracy: 0.6696\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_25!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_45\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_6 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:33:03.023252: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2314 - accuracy: 0.4775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:33:13.002793: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 11s 58ms/step - loss: 1.2314 - accuracy: 0.4775 - val_loss: 1.1487 - val_accuracy: 0.5470\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.1129 - accuracy: 0.5744 - val_loss: 1.0936 - val_accuracy: 0.5954\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 1.8871 - accuracy: 0.4654 - val_loss: 3.6502 - val_accuracy: 0.3345\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.8261 - accuracy: 0.5000 - val_loss: 1.3369 - val_accuracy: 0.5320\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 59ms/step - loss: 1.1105 - accuracy: 0.6046 - val_loss: 1.2031 - val_accuracy: 0.5886\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 1.5038 - accuracy: 0.6039 - val_loss: 3.0472 - val_accuracy: 0.4721\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 62ms/step - loss: 4.3182 - accuracy: 0.5424 - val_loss: 4.2727 - val_accuracy: 0.5640\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 11.6133 - accuracy: 0.5410 - val_loss: 19.0241 - val_accuracy: 0.5395\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 27.9783 - accuracy: 0.5390 - val_loss: 34.5470 - val_accuracy: 0.5586\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 12s 62ms/step - loss: 101.3802 - accuracy: 0.5092 - val_loss: 221.4466 - val_accuracy: 0.4700\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_45!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_90\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_7 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:34:55.250133: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.3920 - accuracy: 0.3890"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:35:06.030133: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 12s 62ms/step - loss: 1.3920 - accuracy: 0.3890 - val_loss: 1.1514 - val_accuracy: 0.5198\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 1.1294 - accuracy: 0.5708 - val_loss: 1.4993 - val_accuracy: 0.5286\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 1.1099 - accuracy: 0.5899 - val_loss: 1.2110 - val_accuracy: 0.5988\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 1.0963 - accuracy: 0.6357 - val_loss: 1.2877 - val_accuracy: 0.6144\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 1.1482 - accuracy: 0.6657 - val_loss: 2.2567 - val_accuracy: 0.5361\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 2.0696 - accuracy: 0.6284 - val_loss: 3.4573 - val_accuracy: 0.5545\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 11s 61ms/step - loss: 4.0085 - accuracy: 0.6207 - val_loss: 6.3788 - val_accuracy: 0.5477\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 61ms/step - loss: 4.8337 - accuracy: 0.6626 - val_loss: 12.2085 - val_accuracy: 0.5395\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 61ms/step - loss: 17.8032 - accuracy: 0.6013 - val_loss: 35.4132 - val_accuracy: 0.5416\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 61ms/step - loss: 77.0289 - accuracy: 0.5877 - val_loss: 239.5297 - val_accuracy: 0.4394\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_90!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_zoom_zoom0_5\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_8 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_24 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:36:50.162032: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2703 - accuracy: 0.4859"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:37:00.840296: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 12s 61ms/step - loss: 1.2703 - accuracy: 0.4859 - val_loss: 1.2360 - val_accuracy: 0.5293\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 61ms/step - loss: 1.1854 - accuracy: 0.5952 - val_loss: 1.3878 - val_accuracy: 0.5525\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 61ms/step - loss: 2.4559 - accuracy: 0.5743 - val_loss: 3.5023 - val_accuracy: 0.5456\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 12s 62ms/step - loss: 4.8198 - accuracy: 0.6184 - val_loss: 21.3802 - val_accuracy: 0.3685\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 23.0631 - accuracy: 0.5707 - val_loss: 95.2716 - val_accuracy: 0.4128\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 12s 62ms/step - loss: 56.7862 - accuracy: 0.6078 - val_loss: 127.8810 - val_accuracy: 0.5095\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 63ms/step - loss: 233.7324 - accuracy: 0.5821 - val_loss: 576.4512 - val_accuracy: 0.4639\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 12s 62ms/step - loss: 362.0402 - accuracy: 0.6259 - val_loss: 665.9569 - val_accuracy: 0.5007\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 12s 63ms/step - loss: 1064.4531 - accuracy: 0.5603 - val_loss: 1230.9637 - val_accuracy: 0.5095\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 12s 63ms/step - loss: 2135.2610 - accuracy: 0.5611 - val_loss: 4422.9512 - val_accuracy: 0.5232\n",
      " Modelo treinado e salvo para augmented_flower_photos_zoom_zoom0_5!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_zoom_zoom0_10\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_9 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_29 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:38:46.351892: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2750 - accuracy: 0.4823"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:38:57.306542: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 12s 63ms/step - loss: 1.2750 - accuracy: 0.4823 - val_loss: 1.0763 - val_accuracy: 0.5681\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 12s 63ms/step - loss: 1.0014 - accuracy: 0.6161 - val_loss: 1.1036 - val_accuracy: 0.5892\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 12s 64ms/step - loss: 0.8023 - accuracy: 0.7119 - val_loss: 1.1515 - val_accuracy: 0.6219\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 12s 63ms/step - loss: 0.6840 - accuracy: 0.7686 - val_loss: 1.3336 - val_accuracy: 0.6628\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 0.9692 - accuracy: 0.7818 - val_loss: 3.1496 - val_accuracy: 0.5811\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 0.6646 - accuracy: 0.8345 - val_loss: 6.7921 - val_accuracy: 0.4394\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 0.6234 - accuracy: 0.8609 - val_loss: 2.4724 - val_accuracy: 0.6866\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 0.8604 - accuracy: 0.8687 - val_loss: 3.9528 - val_accuracy: 0.7234\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 2.4462 - accuracy: 0.8350 - val_loss: 8.2716 - val_accuracy: 0.7098\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 8.8977 - accuracy: 0.7992 - val_loss: 31.7917 - val_accuracy: 0.6444\n",
      " Modelo treinado e salvo para augmented_flower_photos_zoom_zoom0_10!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_zoom_zoom0_20\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_10 (Rescaling)    (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_30 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_31 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_32 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:40:47.613697: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.3582 - accuracy: 0.4455"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:40:58.916688: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 12s 65ms/step - loss: 1.3582 - accuracy: 0.4455 - val_loss: 1.0889 - val_accuracy: 0.5674\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 12s 64ms/step - loss: 1.3203 - accuracy: 0.5467 - val_loss: 1.6399 - val_accuracy: 0.5136\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 1.8644 - accuracy: 0.5708 - val_loss: 3.1295 - val_accuracy: 0.5150\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 3.0607 - accuracy: 0.5693 - val_loss: 5.6368 - val_accuracy: 0.4183\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 5.1183 - accuracy: 0.6148 - val_loss: 8.5841 - val_accuracy: 0.5872\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 12.3324 - accuracy: 0.6119 - val_loss: 27.8467 - val_accuracy: 0.5416\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 61.9495 - accuracy: 0.5760 - val_loss: 106.4303 - val_accuracy: 0.5743\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 191.9853 - accuracy: 0.5770 - val_loss: 602.7302 - val_accuracy: 0.4966\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 657.6848 - accuracy: 0.5644 - val_loss: 1480.6628 - val_accuracy: 0.4659\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 2405.6477 - accuracy: 0.5286 - val_loss: 4853.5254 - val_accuracy: 0.4482\n",
      " Modelo treinado e salvo para augmented_flower_photos_zoom_zoom0_20!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_zoom_zoom0_40\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_11 (Rescaling)    (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_33 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_34 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_34 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_35 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:42:49.635535: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2518 - accuracy: 0.4719"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:43:01.309575: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 13s 67ms/step - loss: 1.2518 - accuracy: 0.4719 - val_loss: 1.1701 - val_accuracy: 0.5341\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 1.3261 - accuracy: 0.5296 - val_loss: 1.1457 - val_accuracy: 0.5429\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 1.0718 - accuracy: 0.6034 - val_loss: 1.1906 - val_accuracy: 0.5422\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 1.2228 - accuracy: 0.5644 - val_loss: 1.2007 - val_accuracy: 0.5770\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 1.2762 - accuracy: 0.5651 - val_loss: 0.9814 - val_accuracy: 0.6015\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 0.9323 - accuracy: 0.6628 - val_loss: 1.0626 - val_accuracy: 0.5845\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 0.7636 - accuracy: 0.7200 - val_loss: 1.0005 - val_accuracy: 0.6458\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 0.6355 - accuracy: 0.7674 - val_loss: 1.2341 - val_accuracy: 0.6301\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 16s 86ms/step - loss: 0.7511 - accuracy: 0.7684 - val_loss: 1.3909 - val_accuracy: 0.6294\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 13s 70ms/step - loss: 0.5238 - accuracy: 0.8231 - val_loss: 1.2826 - val_accuracy: 0.6492\n",
      " Modelo treinado e salvo para augmented_flower_photos_zoom_zoom0_40!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_zoom_zoom0_80\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_12 (Rescaling)    (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_36 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_37 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_38 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:44:57.696648: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2875 - accuracy: 0.4825"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:45:09.198303: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 13s 66ms/step - loss: 1.2875 - accuracy: 0.4825 - val_loss: 1.2527 - val_accuracy: 0.5061\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 1.5886 - accuracy: 0.5330 - val_loss: 4.6468 - val_accuracy: 0.4659\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 5.9195 - accuracy: 0.4865 - val_loss: 10.7589 - val_accuracy: 0.4734\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 12s 67ms/step - loss: 62.5818 - accuracy: 0.4431 - val_loss: 121.0265 - val_accuracy: 0.4087\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 13s 68ms/step - loss: 452.6786 - accuracy: 0.4303 - val_loss: 1009.1532 - val_accuracy: 0.3665\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 2145.4253 - accuracy: 0.3939 - val_loss: 2963.7163 - val_accuracy: 0.4278\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 67ms/step - loss: 6052.9419 - accuracy: 0.3770 - val_loss: 16555.9629 - val_accuracy: 0.2350\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 12s 67ms/step - loss: 12201.6484 - accuracy: 0.3719 - val_loss: 14562.5586 - val_accuracy: 0.3454\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 13s 68ms/step - loss: 19243.6191 - accuracy: 0.3701 - val_loss: 34089.8438 - val_accuracy: 0.3801\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 12s 67ms/step - loss: 30515.1445 - accuracy: 0.3707 - val_loss: 39752.9023 - val_accuracy: 0.3426\n",
      " Modelo treinado e salvo para augmented_flower_photos_zoom_zoom0_80!\n",
      "\n",
      " Todos os modelos foram treinados e salvos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"\\n Treinando modelo para dataset: {dataset}\")\n",
    "\n",
    "    # Carregar o dataset\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dataset,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dataset,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    # Criar o modelo para treinamento\n",
    "    model = keras.Sequential([\n",
    "        layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "        layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Treinar o modelo\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    # Salvar o histÃ³rico de treinamento\n",
    "    training_histories[dataset] = history.history\n",
    "\n",
    "    # Salvar o modelo treinado\n",
    "    model.save(f\"model_{dataset}.h5\")\n",
    "\n",
    "    # Salvar o histÃ³rico de treinamento em um arquivo\n",
    "    with open(f\"history_{dataset}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(history.history, f)\n",
    "\n",
    "    print(f\" Modelo treinado e salvo para {dataset}!\")\n",
    "\n",
    "print(\"\\n Todos os modelos foram treinados e salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc1c9e-27fe-4d64-9752-a1ceb8931b8a",
   "metadata": {},
   "source": [
    "# AcurÃ¡cia final de cada modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b3290f-2fe6-41ca-8eae-14c3c8ac5f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented_flower_photos_flip: 59.40%\n",
      "augmented_flower_photos_rotated_rot0_1: 62.19%\n",
      "augmented_flower_photos_rotated_rot0_5: 67.03%\n",
      "augmented_flower_photos_rotated_rot0_10: 46.46%\n",
      "augmented_flower_photos_rotated_rot0_15: 63.08%\n",
      "augmented_flower_photos_rotated_rot0_25: 66.96%\n",
      "augmented_flower_photos_rotated_rot0_45: 47.00%\n",
      "augmented_flower_photos_rotated_rot0_90: 43.94%\n",
      "augmented_flower_photos_zoom_zoom0_5: 52.32%\n",
      "augmented_flower_photos_zoom_zoom0_10: 64.44%\n",
      "augmented_flower_photos_zoom_zoom0_20: 44.82%\n",
      "augmented_flower_photos_zoom_zoom0_40: 64.92%\n",
      "augmented_flower_photos_zoom_zoom0_80: 34.26%\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    history_file = f\"history_{dataset}.pkl\"\n",
    "\n",
    "    # Verificar se o arquivo de histÃ³rico existe\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"rb\") as f:\n",
    "            history = pickle.load(f)\n",
    "        \n",
    "        # Pegando a Ãºltima acurÃ¡cia no conjunto de validaÃ§Ã£o\n",
    "        final_accuracy = history['val_accuracy'][-1] * 100  # Converte para porcentagem\n",
    "        print(f\"{dataset}: {final_accuracy:.2f}%\")\n",
    "    else:\n",
    "        print(f\"HistÃ³rico nÃ£o encontrado para {dataset}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa312ea9-b4b1-429b-93c9-23e23d1a9822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
