{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183ffe5e-c3f3-43f8-84fb-c6d25e14c83a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tutorial de classificação de imagens TensorFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326d46fd-50b0-482b-be38-ed9ee27da5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b7a46-5191-45ae-8625-6794bf5b06c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09f55d-eefd-4506-9157-462a0fc43ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd16a1-9cdf-4a96-9885-37b57ec08a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data_dir.glob(\"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac681ab-f246-4e98-8f56-5b33a809962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75737b9f-2b54-46f0-b4d9-1d3a355a9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "roses = list(data_dir.glob('roses/*'))\n",
    "PIL.Image.open(str(roses[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7507bc9-d40a-4bde-a200-8456124c3cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(str(roses[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0525e-13ec-499f-9dc4-eb9f1cba350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tulips = list(data_dir.glob('tulips/*'))\n",
    "PIL.Image.open(str(tulips[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6c3b5-1745-4bc9-bbbf-822fc597f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(str(tulips[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c6f81-b5a5-4178-9069-b0b1f60098ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7820f4-842e-4dd7-a016-4c50817d19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb9e5e-6bcd-4ab1-8bfb-bcce6573f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c3ce1-e04c-4183-96e5-ad7f0446cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02ac87-a348-48c5-9155-fac2efd6bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650682e-8e6a-43ad-8eaa-b77554f1a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36629e-f4b8-4e25-9346-03148de628ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e82e8-94a2-4a19-817b-ec6e4e1f0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2088f783-0736-4e05-b9b6-aa824e4ed498",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f19d96-d2ae-4513-85b6-6642e691df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4311e4-f1f1-4603-bc36-e2502be9780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8ac01-78ed-41a9-8689-a81b0f10ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab88f5-1fea-43ee-b14d-c697b51be8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046def2d-5c4b-46c3-848a-95e164f4dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8da61-f54c-4aac-9b5c-638c9496c909",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Criar funções para abrir, exibir, manipular e salvar imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4081c7-e423-495a-bc0f-35dd4770be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979185d-82f6-4031-8a3e-7344bcdf0d6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Flip Horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f6564-46aa-4877-9e42-fa86a551e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_horizontal(image_path, output_path):\n",
    "    \"\"\" Aplica o flip horizontal em uma imagem e salva no caminho especificado. \"\"\"\n",
    "    image_path = os.path.expanduser(image_path)  # Suporte para '~'\n",
    "    output_path = os.path.expanduser(output_path)\n",
    "    \n",
    "    # Carregar a imagem\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"❌ ERRO: Não foi possível carregar a imagem {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Criar pasta de saída, se não existir\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Aplicar flip horizontal\n",
    "    flipped = cv2.flip(image, 1)\n",
    "    \n",
    "    # Salvar a imagem processada\n",
    "    success = cv2.imwrite(output_path, flipped)\n",
    "    if not success:\n",
    "        print(f\"❌ ERRO: Não foi possível salvar a imagem em {output_path}\")\n",
    "    else:\n",
    "        print(f\"✅ Flip horizontal salvo em: {output_path}\")\n",
    "\n",
    "def apply_flip_to_dataset(input_dir, output_dir):\n",
    "    \"\"\" Percorre todas as imagens do dataset e aplica o flip horizontal. \"\"\"\n",
    "    input_dir = os.path.expanduser(input_dir)  # Caminho absoluto\n",
    "    output_dir = os.path.expanduser(output_dir)  # Caminho absoluto\n",
    "    \n",
    "    # Verificar se o diretório de entrada existe\n",
    "    if not os.path.exists(input_dir):\n",
    "        print(f\"❌ ERRO: O diretório {input_dir} não existe!\")\n",
    "        return\n",
    "\n",
    "    # Percorrer todas as classes do dataset\n",
    "    for class_folder in os.listdir(input_dir):\n",
    "        class_input_path = os.path.join(input_dir, class_folder)\n",
    "\n",
    "        # Ignorar arquivos, só processar diretórios (classes)\n",
    "        if not os.path.isdir(class_input_path):\n",
    "            continue\n",
    "\n",
    "        # Criar diretório correspondente na saída\n",
    "        class_output_path = os.path.join(output_dir, class_folder)\n",
    "        os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "        # Percorrer todas as imagens da classe\n",
    "        for image_file in os.listdir(class_input_path):\n",
    "            image_input_path = os.path.join(class_input_path, image_file)\n",
    "            image_output_path = os.path.join(class_output_path, f\"{os.path.splitext(image_file)[0]}_flip.jpg\")\n",
    "\n",
    "            # Aplicar flip horizontal\n",
    "            flip_horizontal(image_input_path, image_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9996d-2e54-4088-a291-1855316f67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar flip horizontal\n",
    "apply_flip_to_dataset(\"~/.keras/datasets/flower_photos\", \"augmented_flower_photos_flip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85699f69-8b58-4879-ab25-9177ded108aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Rotação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a9a0f9-b6a6-4ffe-a637-6a008f47b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(image_path, output_dir, max_angle):\n",
    "    \"\"\" \n",
    "    Rotaciona a imagem por um ângulo aleatório entre 0 e max_angle e salva no diretório de saída.\n",
    "    Preenche pixels indefinidos com preto (RGB 0,0,0).\n",
    "    \"\"\"\n",
    "    image_path = os.path.expanduser(image_path)  # Suporte para '~'\n",
    "    output_dir = os.path.expanduser(output_dir)  # Caminho absoluto para saída\n",
    "\n",
    "    # Carregar imagem\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"❌ ERRO: Falha ao carregar {image_path}\")\n",
    "        return\n",
    "    \n",
    "    h, w = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    angle = random.uniform(0, max_angle)  # Escolher ângulo aleatório entre 0 e max_angle\n",
    "\n",
    "    # Criar matriz de rotação e aplicar a transformação\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, rotation_matrix, (w, h), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "\n",
    "    # Criar caminho de saída mantendo a estrutura do dataset original\n",
    "    class_name = os.path.basename(os.path.dirname(image_path))  # Obtém a classe (ex: 'daisy')\n",
    "    image_name = os.path.basename(image_path)  # Obtém o nome do arquivo original\n",
    "    output_class_dir = os.path.join(output_dir, class_name)  # Criar diretório por classe\n",
    "    os.makedirs(output_class_dir, exist_ok=True)  # Garantir que a pasta existe\n",
    "\n",
    "    # Criar nome do arquivo de saída\n",
    "    output_path = os.path.join(output_class_dir, f\"{os.path.splitext(image_name)[0]}_rot{int(angle)}.jpg\")\n",
    "\n",
    "    # Salvar a imagem rotacionada\n",
    "    success = cv2.imwrite(output_path, rotated)\n",
    "\n",
    "    print(f\"{'✅ Rotação aplicada e salva em ' + output_path if success else '❌ Erro ao salvar a imagem!'}\")\n",
    "\n",
    "def apply_rotation_to_dataset(input_dir, base_output_dir, max_angles=[1, 5, 10, 15, 25, 45, 90]):\n",
    "    \"\"\" \n",
    "    Gera 7 datasets diferentes, aplicando rotação aleatória entre 0 e X graus. \n",
    "    \"\"\"\n",
    "    input_dir = os.path.expanduser(input_dir)  # Caminho absoluto\n",
    "    base_output_dir = os.path.expanduser(base_output_dir)  # Caminho absoluto\n",
    "\n",
    "    # Verificar se o diretório de entrada existe\n",
    "    if not os.path.exists(input_dir):\n",
    "        print(f\"❌ ERRO: O diretório {input_dir} não existe!\")\n",
    "        return\n",
    "\n",
    "    # Criar um dataset separado para cada intervalo de rotação\n",
    "    for max_angle in max_angles:\n",
    "        output_dir = f\"{base_output_dir}_rot0_{max_angle}\"\n",
    "        print(f\"\\n📌 Gerando dataset com rotação entre 0 e {max_angle} graus em: {output_dir}\")\n",
    "        \n",
    "        # Percorrer todas as classes do dataset\n",
    "        for class_folder in os.listdir(input_dir):\n",
    "            class_input_path = os.path.join(input_dir, class_folder)\n",
    "\n",
    "            # Ignorar arquivos, só processar diretórios (classes)\n",
    "            if not os.path.isdir(class_input_path):\n",
    "                continue\n",
    "\n",
    "            # Criar diretório correspondente na saída\n",
    "            class_output_path = os.path.join(output_dir, class_folder)\n",
    "            os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "            # Percorrer todas as imagens da classe\n",
    "            for image_file in os.listdir(class_input_path):\n",
    "                image_input_path = os.path.join(class_input_path, image_file)\n",
    "                rotate_image(image_input_path, output_dir, max_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5310153-8b93-478b-a010-a345b0f10f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar rotação e gerar os 7 datasets\n",
    "apply_rotation_to_dataset(\"~/.keras/datasets/flower_photos\", \"augmented_flower_photos_rotated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f064984-8261-4202-8895-d735f9d32512",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295fc8d1-9a96-4445-afd2-4db7a3461b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_image(image_path, output_dir, max_zoom_factor):\n",
    "    \"\"\" \n",
    "    Aplica zoom aleatório entre 1.0 e max_zoom_factor e corta para manter as dimensões originais.\n",
    "    Salva no diretório de saída mantendo a estrutura do dataset original.\n",
    "    \"\"\"\n",
    "    image_path = os.path.expanduser(image_path)  # Suporte para '~'\n",
    "    output_dir = os.path.expanduser(output_dir)  # Caminho absoluto para saída\n",
    "\n",
    "    # Carregar imagem\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"❌ ERRO: Falha ao carregar {image_path}\")\n",
    "        return\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "    zoom_factor = random.uniform(1.0, max_zoom_factor)  # Escolher um fator de zoom aleatório entre 1 e max_zoom\n",
    "\n",
    "    # Calcular novas dimensões de corte\n",
    "    new_h, new_w = int(h / zoom_factor), int(w / zoom_factor)\n",
    "    start_h = (h - new_h) // 2\n",
    "    start_w = (w - new_w) // 2\n",
    "\n",
    "    # Recortar e redimensionar para manter as dimensões originais\n",
    "    cropped = image[start_h:start_h + new_h, start_w:start_w + new_w]\n",
    "    zoomed = cv2.resize(cropped, (w, h))\n",
    "\n",
    "    # Criar caminho de saída mantendo a estrutura do dataset original\n",
    "    class_name = os.path.basename(os.path.dirname(image_path))  # Obtém a classe (ex: 'daisy')\n",
    "    image_name = os.path.basename(image_path)  # Obtém o nome do arquivo original\n",
    "    output_class_dir = os.path.join(output_dir, class_name)  # Criar diretório por classe\n",
    "    os.makedirs(output_class_dir, exist_ok=True)  # Garantir que a pasta existe\n",
    "\n",
    "    # Criar nome do arquivo de saída\n",
    "    output_path = os.path.join(output_class_dir, f\"{os.path.splitext(image_name)[0]}_zoom{int((zoom_factor - 1) * 100)}.jpg\")\n",
    "\n",
    "    # Salvar a imagem com zoom\n",
    "    success = cv2.imwrite(output_path, zoomed)\n",
    "\n",
    "    print(f\"{'✅ Zoom aplicado e salvo em ' + output_path if success else '❌ Erro ao salvar a imagem!'}\")\n",
    "\n",
    "def apply_zoom_to_dataset(input_dir, base_output_dir, max_zoom_factors=[1.05, 1.10, 1.20, 1.40, 1.80]):\n",
    "    \"\"\" \n",
    "    Gera 5 datasets diferentes, aplicando zoom aleatório entre 1.0 e Y% de zoom.\n",
    "    \"\"\"\n",
    "    input_dir = os.path.expanduser(input_dir)  # Caminho absoluto\n",
    "    base_output_dir = os.path.expanduser(base_output_dir)  # Caminho absoluto\n",
    "\n",
    "    # Verificar se o diretório de entrada existe\n",
    "    if not os.path.exists(input_dir):\n",
    "        print(f\"❌ ERRO: O diretório {input_dir} não existe!\")\n",
    "        return\n",
    "\n",
    "    # Criar um dataset separado para cada intervalo de zoom\n",
    "    for max_zoom in max_zoom_factors:\n",
    "        output_dir = f\"{base_output_dir}_zoom0_{int((max_zoom - 1) * 100)}\"\n",
    "        print(f\"\\n📌 Gerando dataset com zoom entre 0% e {int((max_zoom - 1) * 100)}% em: {output_dir}\")\n",
    "\n",
    "        # Percorrer todas as classes do dataset\n",
    "        for class_folder in os.listdir(input_dir):\n",
    "            class_input_path = os.path.join(input_dir, class_folder)\n",
    "\n",
    "            # Ignorar arquivos, só processar diretórios (classes)\n",
    "            if not os.path.isdir(class_input_path):\n",
    "                continue\n",
    "\n",
    "            # Criar diretório correspondente na saída\n",
    "            class_output_path = os.path.join(output_dir, class_folder)\n",
    "            os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "            # Percorrer todas as imagens da classe\n",
    "            for image_file in os.listdir(class_input_path):\n",
    "                image_input_path = os.path.join(class_input_path, image_file)\n",
    "                zoom_image(image_input_path, output_dir, max_zoom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d7641-3988-4ee0-b70f-535d880da849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar zoom e gerar os 5 datasets\n",
    "apply_zoom_to_dataset(\"~/.keras/datasets/flower_photos\", \"augmented_flower_photos_zoom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7e3fd2-7698-467c-a775-ac8fb8afbfbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Incluir o Dataset original nos Datasets aumentados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68054bec-2659-4d51-abe9-78f48ead9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_original_to_augmented(original_dir, augmented_dirs):\n",
    "    \"\"\"\n",
    "    Copia todas as imagens do dataset original para os datasets aumentados,\n",
    "    garantindo que as classes originais estejam presentes nos aumentos.\n",
    "    \n",
    "    original_dir: Caminho do dataset original (ex: ~/.keras/datasets/flower_photos)\n",
    "    augmented_dirs: Lista de diretórios dos datasets aumentados\n",
    "    \"\"\"\n",
    "    original_dir = os.path.expanduser(original_dir)  # Suporte para '~'\n",
    "    augmented_dirs = [os.path.expanduser(d) for d in augmented_dirs]  # Expande caminhos\n",
    "\n",
    "    # Verifica se o diretório original existe\n",
    "    if not os.path.exists(original_dir):\n",
    "        print(f\"ERRO: O diretório original '{original_dir}' não existe!\")\n",
    "        return\n",
    "    \n",
    "    # Iterar sobre cada dataset aumentado\n",
    "    for augmented_dir in augmented_dirs:\n",
    "        print(f\"\\nCopiando imagens do dataset original para: {augmented_dir}\")\n",
    "\n",
    "        # Criar diretórios das classes no dataset aumentado\n",
    "        for class_folder in os.listdir(original_dir):\n",
    "            class_path_original = os.path.join(original_dir, class_folder)\n",
    "            class_path_augmented = os.path.join(augmented_dir, class_folder)\n",
    "\n",
    "            # Verifica se é um diretório (ignora arquivos)\n",
    "            if not os.path.isdir(class_path_original):\n",
    "                continue\n",
    "\n",
    "            # Cria a pasta no dataset aumentado se não existir\n",
    "            os.makedirs(class_path_augmented, exist_ok=True)\n",
    "\n",
    "            # Copia todas as imagens da classe original para a classe do dataset aumentado\n",
    "            for image_file in os.listdir(class_path_original):\n",
    "                src = os.path.join(class_path_original, image_file)\n",
    "                dst = os.path.join(class_path_augmented, image_file)\n",
    "\n",
    "                # Copiar somente se ainda não estiver no dataset aumentado\n",
    "                if not os.path.exists(dst):\n",
    "                    shutil.copy2(src, dst)\n",
    "\n",
    "        print(f\"Cópia concluída para {augmented_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099d412-a38c-41b7-80ad-4f77411c0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista dos diretórios aumentados\n",
    "augmented_datasets = [\n",
    "    \"augmented_flower_photos_flip\",\n",
    "    \"augmented_flower_photos_rotated_rot0_1\",\n",
    "    \"augmented_flower_photos_rotated_rot0_5\",\n",
    "    \"augmented_flower_photos_rotated_rot0_10\",\n",
    "    \"augmented_flower_photos_rotated_rot0_15\",\n",
    "    \"augmented_flower_photos_rotated_rot0_25\",\n",
    "    \"augmented_flower_photos_rotated_rot0_45\",\n",
    "    \"augmented_flower_photos_rotated_rot0_90\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_5\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_10\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_20\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_40\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_80\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f16abe-6838-469f-ae27-d1eb5e489105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar o dataset original para os datasets aumentados\n",
    "copy_original_to_augmented(\"~/.keras/datasets/flower_photos\", augmented_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a72938-626e-43d6-a96b-e10e93b5c775",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Contagem dos dados dos Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229bfb0-86b6-42b4-9e3a-13c645aeab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images_in_dataset(dataset_path, dataset_name):\n",
    "    \"\"\" Conta a quantidade total de imagens no dataset e exibe os resultados. \"\"\"\n",
    "    dataset_path = os.path.expanduser(dataset_path)  # Garante caminho absoluto\n",
    "    total_images = 0\n",
    "    class_counts = {}  # Contador por classe\n",
    "\n",
    "    # Percorre todas as pastas dentro do dataset\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "\n",
    "        # Ignorar arquivos, só processar diretórios (classes)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        # Conta as imagens dentro da classe\n",
    "        num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        total_images += num_images\n",
    "        class_counts[class_folder] = num_images\n",
    "\n",
    "    # Exibir resultados\n",
    "    print(f\"\\n**Resumo do dataset: {dataset_name}**\")\n",
    "    print(f\"   Total de imagens: {total_images}\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"   Classe '{class_name}': {count} imagens\")\n",
    "\n",
    "    return total_images, class_counts\n",
    "\n",
    "# Contar imagens no dataset original\n",
    "original_total, original_counts = count_images_in_dataset(\"~/.keras/datasets/flower_photos\", \"Original\")\n",
    "\n",
    "# Contar imagens no dataset aumentado\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_flip\", \"Flip\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_1\", \"Rotação x a 1º\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_5\", \"Rotação x a 5º\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_10\", \"Rotação x a 10º\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_15\", \"Rotação x a 15º\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_25\", \"Rotação x a 25º\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_45\", \"Rotação x a 45º\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_rotated_rot0_90\", \"Rotação x a 90º\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_zoom_zoom0_5\", \"Zoom y a 5%\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_zoom_zoom0_10\", \"Zoom y a 10%\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_zoom_zoom0_19\", \"Zoom y a 20%\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_zoom_zoom0_39\", \"Zoom y a 40%\")\n",
    "augmented_total, augmented_counts = count_images_in_dataset(\"augmented_flower_photos_zoom_zoom0_80\", \"Zoom y a 80%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ab56d-55e4-4bff-abd4-a2475a9c3461",
   "metadata": {},
   "source": [
    "# TREINAMENTO DE TODOS OS DATASETS (FLIP, ROTAÇÃO E ZOOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "273c3e01-4c58-49bd-8570-806a852f3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c106622-fe34-4a83-8e46-d619eb5e6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de datasets para Flip, Rotação e Zoom\n",
    "datasets = [\n",
    "    \"augmented_flower_photos_flip\",\n",
    "    \"augmented_flower_photos_rotated_rot0_1\",\n",
    "    \"augmented_flower_photos_rotated_rot0_5\",\n",
    "    \"augmented_flower_photos_rotated_rot0_10\",\n",
    "    \"augmented_flower_photos_rotated_rot0_15\",\n",
    "    \"augmented_flower_photos_rotated_rot0_25\",\n",
    "    \"augmented_flower_photos_rotated_rot0_45\",\n",
    "    \"augmented_flower_photos_rotated_rot0_90\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_5\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_10\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_20\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_40\",\n",
    "    \"augmented_flower_photos_zoom_zoom0_80\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2cfb6c-9cb1-4f7f-abec-ada33b9ce6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um dicionário para armazenar os históricos de treinamento\n",
    "training_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e93424-18c8-4660-851e-a7f95a05d87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_flip\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:22:13.849172: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-02-04 22:22:13.849205: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-02-04 22:22:13.849214: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-02-04 22:22:13.849781: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-04 22:22:13.850039: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 90, 90, 16)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 45, 45, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 22, 22, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 30976)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:22:14.632766: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2299 - accuracy: 0.5043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:22:25.165607: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 12s 60ms/step - loss: 1.2299 - accuracy: 0.5043 - val_loss: 1.0129 - val_accuracy: 0.5743\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.9610 - accuracy: 0.6323 - val_loss: 0.9694 - val_accuracy: 0.6213\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.8755 - accuracy: 0.6912 - val_loss: 1.2607 - val_accuracy: 0.5756\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.8323 - accuracy: 0.7236 - val_loss: 1.6633 - val_accuracy: 0.5995\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 0.9708 - accuracy: 0.7147 - val_loss: 1.8604 - val_accuracy: 0.6015\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.9929 - accuracy: 0.7706 - val_loss: 3.2001 - val_accuracy: 0.5790\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 2.2528 - accuracy: 0.7360 - val_loss: 6.4109 - val_accuracy: 0.4966\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 3.4169 - accuracy: 0.7435 - val_loss: 10.4076 - val_accuracy: 0.6069\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 13.3519 - accuracy: 0.6851 - val_loss: 39.3122 - val_accuracy: 0.5388\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 32.4304 - accuracy: 0.7059 - val_loss: 74.4513 - val_accuracy: 0.5940\n",
      " Modelo treinado e salvo para augmented_flower_photos_flip!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moa/Desktop/tensorflow-test/env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_1 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 90, 90, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 45, 45, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 22, 22, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:24:02.491052: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.1804 - accuracy: 0.5078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:24:12.416741: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 11s 57ms/step - loss: 1.1804 - accuracy: 0.5078 - val_loss: 0.9683 - val_accuracy: 0.6022\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.9634 - accuracy: 0.6500 - val_loss: 1.1944 - val_accuracy: 0.5525\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.8304 - accuracy: 0.7079 - val_loss: 1.4036 - val_accuracy: 0.6253\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.9319 - accuracy: 0.7284 - val_loss: 1.4981 - val_accuracy: 0.6471\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.4365 - accuracy: 0.7554 - val_loss: 2.2894 - val_accuracy: 0.7064\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 10s 56ms/step - loss: 2.0599 - accuracy: 0.7670 - val_loss: 5.6601 - val_accuracy: 0.6526\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 10s 56ms/step - loss: 5.6279 - accuracy: 0.7580 - val_loss: 24.2675 - val_accuracy: 0.6396\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 18.8783 - accuracy: 0.7050 - val_loss: 37.6731 - val_accuracy: 0.6676\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 10s 56ms/step - loss: 86.1298 - accuracy: 0.6819 - val_loss: 268.2289 - val_accuracy: 0.5743\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 254.3374 - accuracy: 0.6991 - val_loss: 644.2495 - val_accuracy: 0.6219\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_1!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_5\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_2 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 90, 90, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 45, 45, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 22, 22, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:25:49.041904: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2067 - accuracy: 0.4986"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:25:59.128766: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 11s 58ms/step - loss: 1.2067 - accuracy: 0.4986 - val_loss: 0.9501 - val_accuracy: 0.6240\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.8787 - accuracy: 0.6730 - val_loss: 0.9848 - val_accuracy: 0.6144\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.7367 - accuracy: 0.7444 - val_loss: 1.0183 - val_accuracy: 0.6996\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.7737 - accuracy: 0.7856 - val_loss: 1.9454 - val_accuracy: 0.6540\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.3178 - accuracy: 0.7815 - val_loss: 3.5480 - val_accuracy: 0.6635\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 2.7308 - accuracy: 0.7638 - val_loss: 4.9710 - val_accuracy: 0.7078\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 5.1150 - accuracy: 0.7670 - val_loss: 14.9564 - val_accuracy: 0.6471\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 9.1280 - accuracy: 0.7905 - val_loss: 21.6273 - val_accuracy: 0.6975\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 20.7521 - accuracy: 0.7800 - val_loss: 55.4164 - val_accuracy: 0.6737\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 37.8027 - accuracy: 0.7967 - val_loss: 97.2537 - val_accuracy: 0.6703\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_5!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_10\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_3 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 90, 90, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:27:36.068876: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.3827 - accuracy: 0.4264"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:27:46.122590: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 11s 58ms/step - loss: 1.3827 - accuracy: 0.4264 - val_loss: 1.0668 - val_accuracy: 0.5586\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.0649 - accuracy: 0.5947 - val_loss: 1.1794 - val_accuracy: 0.5940\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.4537 - accuracy: 0.6095 - val_loss: 4.2463 - val_accuracy: 0.3583\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.0612 - accuracy: 0.6608 - val_loss: 1.1567 - val_accuracy: 0.6131\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 1.0407 - accuracy: 0.7042 - val_loss: 1.9287 - val_accuracy: 0.6042\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 3.5180 - accuracy: 0.5991 - val_loss: 2.1375 - val_accuracy: 0.6308\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 10s 56ms/step - loss: 7.9711 - accuracy: 0.5482 - val_loss: 30.6365 - val_accuracy: 0.4230\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 19.2702 - accuracy: 0.5450 - val_loss: 20.4133 - val_accuracy: 0.5245\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 59ms/step - loss: 29.5520 - accuracy: 0.5545 - val_loss: 65.9434 - val_accuracy: 0.4952\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 220.4574 - accuracy: 0.4838 - val_loss: 773.5009 - val_accuracy: 0.4646\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_10!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_15\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_4 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:29:24.341620: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.1939 - accuracy: 0.5112"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:29:34.500391: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 11s 59ms/step - loss: 1.1939 - accuracy: 0.5112 - val_loss: 1.0733 - val_accuracy: 0.5661\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 1.1635 - accuracy: 0.5567 - val_loss: 1.3312 - val_accuracy: 0.5184\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 1.3266 - accuracy: 0.6005 - val_loss: 4.7160 - val_accuracy: 0.3549\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.6628 - accuracy: 0.5662 - val_loss: 1.5293 - val_accuracy: 0.5845\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.9674 - accuracy: 0.5358 - val_loss: 2.5166 - val_accuracy: 0.4407\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 2.1178 - accuracy: 0.6008 - val_loss: 5.4276 - val_accuracy: 0.4932\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 4.5952 - accuracy: 0.5816 - val_loss: 7.1761 - val_accuracy: 0.5456\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 7.6208 - accuracy: 0.6199 - val_loss: 11.3995 - val_accuracy: 0.5831\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 7.7410 - accuracy: 0.6776 - val_loss: 21.9437 - val_accuracy: 0.5307\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 17.3644 - accuracy: 0.6403 - val_loss: 23.5795 - val_accuracy: 0.6308\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_15!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_25\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_5 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:31:13.075566: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.3261 - accuracy: 0.4557"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:31:23.139102: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 11s 58ms/step - loss: 1.3261 - accuracy: 0.4557 - val_loss: 1.1556 - val_accuracy: 0.5572\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.0087 - accuracy: 0.6075 - val_loss: 1.0204 - val_accuracy: 0.6042\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 60ms/step - loss: 0.8542 - accuracy: 0.6761 - val_loss: 0.9632 - val_accuracy: 0.6410\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 0.7096 - accuracy: 0.7347 - val_loss: 1.4571 - val_accuracy: 0.5913\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 59ms/step - loss: 0.5814 - accuracy: 0.7946 - val_loss: 1.0434 - val_accuracy: 0.6710\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 59ms/step - loss: 0.4641 - accuracy: 0.8415 - val_loss: 1.7079 - val_accuracy: 0.6199\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 11s 59ms/step - loss: 0.6590 - accuracy: 0.8161 - val_loss: 1.6455 - val_accuracy: 0.6471\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 59ms/step - loss: 0.4688 - accuracy: 0.8672 - val_loss: 2.5120 - val_accuracy: 0.6151\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.3783 - accuracy: 0.8942 - val_loss: 2.0346 - val_accuracy: 0.6757\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 0.9572 - accuracy: 0.8435 - val_loss: 3.5586 - val_accuracy: 0.6696\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_25!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_45\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_6 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:33:03.023252: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2314 - accuracy: 0.4775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:33:13.002793: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 11s 58ms/step - loss: 1.2314 - accuracy: 0.4775 - val_loss: 1.1487 - val_accuracy: 0.5470\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.1129 - accuracy: 0.5744 - val_loss: 1.0936 - val_accuracy: 0.5954\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 1.8871 - accuracy: 0.4654 - val_loss: 3.6502 - val_accuracy: 0.3345\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 1.8261 - accuracy: 0.5000 - val_loss: 1.3369 - val_accuracy: 0.5320\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 59ms/step - loss: 1.1105 - accuracy: 0.6046 - val_loss: 1.2031 - val_accuracy: 0.5886\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 1.5038 - accuracy: 0.6039 - val_loss: 3.0472 - val_accuracy: 0.4721\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 62ms/step - loss: 4.3182 - accuracy: 0.5424 - val_loss: 4.2727 - val_accuracy: 0.5640\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 11.6133 - accuracy: 0.5410 - val_loss: 19.0241 - val_accuracy: 0.5395\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 27.9783 - accuracy: 0.5390 - val_loss: 34.5470 - val_accuracy: 0.5586\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 12s 62ms/step - loss: 101.3802 - accuracy: 0.5092 - val_loss: 221.4466 - val_accuracy: 0.4700\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_45!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_rotated_rot0_90\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_7 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:34:55.250133: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.3920 - accuracy: 0.3890"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:35:06.030133: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 12s 62ms/step - loss: 1.3920 - accuracy: 0.3890 - val_loss: 1.1514 - val_accuracy: 0.5198\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 1.1294 - accuracy: 0.5708 - val_loss: 1.4993 - val_accuracy: 0.5286\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 1.1099 - accuracy: 0.5899 - val_loss: 1.2110 - val_accuracy: 0.5988\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 1.0963 - accuracy: 0.6357 - val_loss: 1.2877 - val_accuracy: 0.6144\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 1.1482 - accuracy: 0.6657 - val_loss: 2.2567 - val_accuracy: 0.5361\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 2.0696 - accuracy: 0.6284 - val_loss: 3.4573 - val_accuracy: 0.5545\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 11s 61ms/step - loss: 4.0085 - accuracy: 0.6207 - val_loss: 6.3788 - val_accuracy: 0.5477\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 61ms/step - loss: 4.8337 - accuracy: 0.6626 - val_loss: 12.2085 - val_accuracy: 0.5395\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 61ms/step - loss: 17.8032 - accuracy: 0.6013 - val_loss: 35.4132 - val_accuracy: 0.5416\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 61ms/step - loss: 77.0289 - accuracy: 0.5877 - val_loss: 239.5297 - val_accuracy: 0.4394\n",
      " Modelo treinado e salvo para augmented_flower_photos_rotated_rot0_90!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_zoom_zoom0_5\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_8 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_24 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:36:50.162032: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2703 - accuracy: 0.4859"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:37:00.840296: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 12s 61ms/step - loss: 1.2703 - accuracy: 0.4859 - val_loss: 1.2360 - val_accuracy: 0.5293\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 11s 61ms/step - loss: 1.1854 - accuracy: 0.5952 - val_loss: 1.3878 - val_accuracy: 0.5525\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 61ms/step - loss: 2.4559 - accuracy: 0.5743 - val_loss: 3.5023 - val_accuracy: 0.5456\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 12s 62ms/step - loss: 4.8198 - accuracy: 0.6184 - val_loss: 21.3802 - val_accuracy: 0.3685\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 62ms/step - loss: 23.0631 - accuracy: 0.5707 - val_loss: 95.2716 - val_accuracy: 0.4128\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 12s 62ms/step - loss: 56.7862 - accuracy: 0.6078 - val_loss: 127.8810 - val_accuracy: 0.5095\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 63ms/step - loss: 233.7324 - accuracy: 0.5821 - val_loss: 576.4512 - val_accuracy: 0.4639\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 12s 62ms/step - loss: 362.0402 - accuracy: 0.6259 - val_loss: 665.9569 - val_accuracy: 0.5007\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 12s 63ms/step - loss: 1064.4531 - accuracy: 0.5603 - val_loss: 1230.9637 - val_accuracy: 0.5095\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 12s 63ms/step - loss: 2135.2610 - accuracy: 0.5611 - val_loss: 4422.9512 - val_accuracy: 0.5232\n",
      " Modelo treinado e salvo para augmented_flower_photos_zoom_zoom0_5!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_zoom_zoom0_10\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_9 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_29 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:38:46.351892: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2750 - accuracy: 0.4823"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:38:57.306542: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 12s 63ms/step - loss: 1.2750 - accuracy: 0.4823 - val_loss: 1.0763 - val_accuracy: 0.5681\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 12s 63ms/step - loss: 1.0014 - accuracy: 0.6161 - val_loss: 1.1036 - val_accuracy: 0.5892\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 12s 64ms/step - loss: 0.8023 - accuracy: 0.7119 - val_loss: 1.1515 - val_accuracy: 0.6219\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 12s 63ms/step - loss: 0.6840 - accuracy: 0.7686 - val_loss: 1.3336 - val_accuracy: 0.6628\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 0.9692 - accuracy: 0.7818 - val_loss: 3.1496 - val_accuracy: 0.5811\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 0.6646 - accuracy: 0.8345 - val_loss: 6.7921 - val_accuracy: 0.4394\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 0.6234 - accuracy: 0.8609 - val_loss: 2.4724 - val_accuracy: 0.6866\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 0.8604 - accuracy: 0.8687 - val_loss: 3.9528 - val_accuracy: 0.7234\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 2.4462 - accuracy: 0.8350 - val_loss: 8.2716 - val_accuracy: 0.7098\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 8.8977 - accuracy: 0.7992 - val_loss: 31.7917 - val_accuracy: 0.6444\n",
      " Modelo treinado e salvo para augmented_flower_photos_zoom_zoom0_10!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_zoom_zoom0_20\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_10 (Rescaling)    (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_30 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_31 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_32 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:40:47.613697: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.3582 - accuracy: 0.4455"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:40:58.916688: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 12s 65ms/step - loss: 1.3582 - accuracy: 0.4455 - val_loss: 1.0889 - val_accuracy: 0.5674\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 12s 64ms/step - loss: 1.3203 - accuracy: 0.5467 - val_loss: 1.6399 - val_accuracy: 0.5136\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 1.8644 - accuracy: 0.5708 - val_loss: 3.1295 - val_accuracy: 0.5150\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 3.0607 - accuracy: 0.5693 - val_loss: 5.6368 - val_accuracy: 0.4183\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 5.1183 - accuracy: 0.6148 - val_loss: 8.5841 - val_accuracy: 0.5872\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 12.3324 - accuracy: 0.6119 - val_loss: 27.8467 - val_accuracy: 0.5416\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 61.9495 - accuracy: 0.5760 - val_loss: 106.4303 - val_accuracy: 0.5743\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 191.9853 - accuracy: 0.5770 - val_loss: 602.7302 - val_accuracy: 0.4966\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 657.6848 - accuracy: 0.5644 - val_loss: 1480.6628 - val_accuracy: 0.4659\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 2405.6477 - accuracy: 0.5286 - val_loss: 4853.5254 - val_accuracy: 0.4482\n",
      " Modelo treinado e salvo para augmented_flower_photos_zoom_zoom0_20!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_zoom_zoom0_40\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_11 (Rescaling)    (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_33 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_34 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_34 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_35 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:42:49.635535: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2518 - accuracy: 0.4719"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:43:01.309575: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 13s 67ms/step - loss: 1.2518 - accuracy: 0.4719 - val_loss: 1.1701 - val_accuracy: 0.5341\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 1.3261 - accuracy: 0.5296 - val_loss: 1.1457 - val_accuracy: 0.5429\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 1.0718 - accuracy: 0.6034 - val_loss: 1.1906 - val_accuracy: 0.5422\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 1.2228 - accuracy: 0.5644 - val_loss: 1.2007 - val_accuracy: 0.5770\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 1.2762 - accuracy: 0.5651 - val_loss: 0.9814 - val_accuracy: 0.6015\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 0.9323 - accuracy: 0.6628 - val_loss: 1.0626 - val_accuracy: 0.5845\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 0.7636 - accuracy: 0.7200 - val_loss: 1.0005 - val_accuracy: 0.6458\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 0.6355 - accuracy: 0.7674 - val_loss: 1.2341 - val_accuracy: 0.6301\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 16s 86ms/step - loss: 0.7511 - accuracy: 0.7684 - val_loss: 1.3909 - val_accuracy: 0.6294\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 13s 70ms/step - loss: 0.5238 - accuracy: 0.8231 - val_loss: 1.2826 - val_accuracy: 0.6492\n",
      " Modelo treinado e salvo para augmented_flower_photos_zoom_zoom0_40!\n",
      "\n",
      " Treinando modelo para dataset: augmented_flower_photos_zoom_zoom0_80\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 5872 files for training.\n",
      "Found 7340 files belonging to 5 classes.\n",
      "Using 1468 files for validation.\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_12 (Rescaling)    (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, 180, 180, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_36 (MaxPooli  (None, 90, 90, 16)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_37 (MaxPooli  (None, 45, 45, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_38 (MaxPooli  (None, 22, 22, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               3965056   \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3989285 (15.22 MB)\n",
      "Trainable params: 3989285 (15.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:44:57.696648: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - ETA: 0s - loss: 1.2875 - accuracy: 0.4825"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 22:45:09.198303: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 13s 66ms/step - loss: 1.2875 - accuracy: 0.4825 - val_loss: 1.2527 - val_accuracy: 0.5061\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 1.5886 - accuracy: 0.5330 - val_loss: 4.6468 - val_accuracy: 0.4659\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 5.9195 - accuracy: 0.4865 - val_loss: 10.7589 - val_accuracy: 0.4734\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 12s 67ms/step - loss: 62.5818 - accuracy: 0.4431 - val_loss: 121.0265 - val_accuracy: 0.4087\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 13s 68ms/step - loss: 452.6786 - accuracy: 0.4303 - val_loss: 1009.1532 - val_accuracy: 0.3665\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 2145.4253 - accuracy: 0.3939 - val_loss: 2963.7163 - val_accuracy: 0.4278\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 67ms/step - loss: 6052.9419 - accuracy: 0.3770 - val_loss: 16555.9629 - val_accuracy: 0.2350\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 12s 67ms/step - loss: 12201.6484 - accuracy: 0.3719 - val_loss: 14562.5586 - val_accuracy: 0.3454\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 13s 68ms/step - loss: 19243.6191 - accuracy: 0.3701 - val_loss: 34089.8438 - val_accuracy: 0.3801\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 12s 67ms/step - loss: 30515.1445 - accuracy: 0.3707 - val_loss: 39752.9023 - val_accuracy: 0.3426\n",
      " Modelo treinado e salvo para augmented_flower_photos_zoom_zoom0_80!\n",
      "\n",
      " Todos os modelos foram treinados e salvos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"\\n Treinando modelo para dataset: {dataset}\")\n",
    "\n",
    "    # Carregar o dataset\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dataset,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dataset,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    # Criar o modelo para treinamento\n",
    "    model = keras.Sequential([\n",
    "        layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "        layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Treinar o modelo\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    # Salvar o histórico de treinamento\n",
    "    training_histories[dataset] = history.history\n",
    "\n",
    "    # Salvar o modelo treinado\n",
    "    model.save(f\"model_{dataset}.h5\")\n",
    "\n",
    "    # Salvar o histórico de treinamento em um arquivo\n",
    "    with open(f\"history_{dataset}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(history.history, f)\n",
    "\n",
    "    print(f\" Modelo treinado e salvo para {dataset}!\")\n",
    "\n",
    "print(\"\\n Todos os modelos foram treinados e salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc1c9e-27fe-4d64-9752-a1ceb8931b8a",
   "metadata": {},
   "source": [
    "# Acurácia final de cada modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b3290f-2fe6-41ca-8eae-14c3c8ac5f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented_flower_photos_flip: 59.40%\n",
      "augmented_flower_photos_rotated_rot0_1: 62.19%\n",
      "augmented_flower_photos_rotated_rot0_5: 67.03%\n",
      "augmented_flower_photos_rotated_rot0_10: 46.46%\n",
      "augmented_flower_photos_rotated_rot0_15: 63.08%\n",
      "augmented_flower_photos_rotated_rot0_25: 66.96%\n",
      "augmented_flower_photos_rotated_rot0_45: 47.00%\n",
      "augmented_flower_photos_rotated_rot0_90: 43.94%\n",
      "augmented_flower_photos_zoom_zoom0_5: 52.32%\n",
      "augmented_flower_photos_zoom_zoom0_10: 64.44%\n",
      "augmented_flower_photos_zoom_zoom0_20: 44.82%\n",
      "augmented_flower_photos_zoom_zoom0_40: 64.92%\n",
      "augmented_flower_photos_zoom_zoom0_80: 34.26%\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    history_file = f\"history_{dataset}.pkl\"\n",
    "\n",
    "    # Verificar se o arquivo de histórico existe\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"rb\") as f:\n",
    "            history = pickle.load(f)\n",
    "        \n",
    "        # Pegando a última acurácia no conjunto de validação\n",
    "        final_accuracy = history['val_accuracy'][-1] * 100  # Converte para porcentagem\n",
    "        print(f\"{dataset}: {final_accuracy:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Histórico não encontrado para {dataset}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa312ea9-b4b1-429b-93c9-23e23d1a9822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
